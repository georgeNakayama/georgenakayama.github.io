<head>
    <title>Kiyohiro Nakayama</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="resources/main.css">
</head>

<body>

<table>
<tr>
<td><img src="images/avatar.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Kiyohiro (George) Nakayama 中山聖博</div>
<div>
Master's Student at Stanford University<br/>
</div>
<div>
    <b>Email:</b> <tt>w4756677 [at] stanford [dot] com</tt><br>
</div>
<div>
<a href="resources/Resume.pdf" target='_blank'>[CV (November, 2024)]</a>&nbsp;
<a href="#project">[Projects]</a>&nbsp;
<a href="#education">[Education]</a>&nbsp;
<a href="#experiences">[Experiences]</a>&nbsp;
<br><br>
<!-- <a href="https://scholar.google.com/citations?user=pL7JsOsAAAAJ&hl=en" target='_blank'>[Google Scholar]</a>&nbsp; -->
<a href="https://github.com/georgeNakayama" target='_blank'>[GitHub]</a>
<a href="https://www.linkedin.com/in/kiyohiro-nakayama-473249123/" target="_blank">[LinkedIn]</a>
<!-- <a href="https://twitter.com/KaichunMo" target="_blank">[Twitter]</a> -->
</div>
</td>
</tr>
</table>
<br>

<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>"It isn't that we dream too wild a dream: <br>
        The trouble is we do not make it seem <br>
        Sufficiently unlikely; for the most <br>
        We can think up is a domestic ghost."</i></p>
        <p>--- Vladmir Nabokov <i>Pale Fire</i></p>
</div>

<h3>About</h3>
<div class="section">
<ul>
    <p>
        I am a Coterm Master's student at Stanford University majoring in mathematics and computer science.I am generally interested in computer vision, graphics and machine learning. My recent research focuses on the representation and generation of objects/scenes for 2D and 3D content creation. 
    </p>

    <p>
    I am fortunate to work under the supervision of 
    <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Prof. Leonidas J. Guibas</a>, 
    <a href="https://stanford.edu/~gordonwz/" target="_blank">Prof. Gordon Wetzstein</a>, and 
    <a href="https://graphics.stanford.edu/~djames/" target="_blank">Prof. Doug L. James</a>.

    I am mentored by 
    <a href="https://mikacuy.github.io/" target="_blank">Mikaela Angelina Uy</a> and <a href="https://qq456cvb.github.io/" target="_blank">Yang You</a>, and <a href="https://www.guandaoyang.com/" target="_blank">Guandao Yang</a>  whose help and support I cannot appreciate enough. 

    I was also a visiting student at 
<a href="https://www.tsinghua.edu.cn/en/" target='_blank'>Tsinghua University</a>
    under the supervision of 
    <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target='_blank'>Prof. Shi-Min Hu</a>.
    There, I am mentored by two wonderful mentors 
    <a href="https://cg.cs.tsinghua.edu.cn/people/~huangjh/" target="_blank">Jiahui Huang</a> and Guo-Ye Yang.
    </p>
</div>
<!-- <br>

<h3>News</h3>
<div class="section">
<ul>
<ul>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Apr, 2023] One paper accepted to ICML 2023.</li>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Mar, 2023] We will host <a href="https://ai3dcc.github.io/" target="_blank">The First Workshop on AI for 3D Content Creation (AI3DCC)</a> at ICCV 2023.</li>
    <li>[Feb, 2023] Call-for-Papers for CVPR 2023 workshops: 
            <a href="https://struco3d.github.io/cvpr2023" target="_blank">Structural and Compositional Learning on 3D Data (StruCo3D)</a>,
            <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">3D Vision and Robotics (3DVR)</a>, and 
            <a href="https://rhobin-challenge.github.io/" target="_blank">The Rhobin Challenge -- Reconstruction of human-object interaction</a>.
    </li>
    <li>[Feb, 2023] Papers accepted to CVPR 2023, ICLR 2023, TPAMI, and WACV 2023.</li>
    <li>[Jul, 2022] Two papers get accepted to ECCV 2022.</li>
    <li>[Jun, 2022] Successfully defended <a href="https://purl.stanford.edu/xn613mb1512" target="_blank">my Ph.D. thesis</a> (featured in <a href="https://www.rsipvision.com/ComputerVisionNews-2022June/22/" target="_blank">Computer Vision News magazine</a>).</li>
    <li>Our ECCV22 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/voli/" target="_blank">VOLI: Visual Object-oriented Learning meets Interaction: Discovery, Representations, and Applications</a>. Please consider submitting your works by July 5th, 2022. See you online in Oct, 2022.</li>
    <li>One paper gets accepted to CVPR 2022.</li>
    <li>[Jan, 2022] Three papers get accepted to ICLR 2022.</li>
    <li>[Sept, 2021] Two papers get accepted to CoRL 2021. My first ever papers in any Robotic Conference!</li>
    <li>[July, 2021] One paper gets accepted to ICCV 2021.</li>
    <li>[Apr, 2021] 
        Our ICCV2021 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/struco3d/" target="_blank">StruCo3D2021: Structural and Compositional Learning on 3D Data</a>. Please consider submitting your works by July 26, 2021. See you online on Oct 16th, 2021.</li>
    <li>[Sep, 2020] One paper gets accepted to NeurIPS 2020.</li>
    <li>[July, 2020] Two papers get accepted to ECCV 2020.</li>
    <li>[Feb, 2020] Two papers get accepted to CVPR 2020.</li>
    <li>[Dec, 2019] One paper gets accepted to ICLR 2020.</li>
    <li>[July, 2019] StructureNet gets accepted to Siggraph Asia 2019!</li>
    <li>[Feb, 2019] PartNet gets accepted to CVPR 2019!</li>
    <li>[February 2017] Our <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">PointNet</a> is accepted as an oral presentation in CVPR 2017, Honolulu, USA.</li>
</ul>
</div>
<br> -->




<a name="project"></a>
<h3>Research</h3>
<div class="mainsection">
<ul>

<!-- (<sup>*</sup>: indicates equal contribution.) -->

<table width="100%">
<!--  ProvNeRF -->
<tr>
    <td width="30%" valign="top"><p><img src="papers/provNeRF.png" width="300" alt="" style="border-style: none" align="top"></p></td>
    <td width="70%" valign="top"><p>
        <h3>
            ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process
        </h3>
        <strong>Kiyohiro Nakayama</strong>,
        <a href="https://mikacuy.github.io/" target="_blank">Mikaela Angelina Uy</a>,
        <a href="https://qq456cvb.github.io/" target="_blank">Yang You</a>,
        <a href="http://www.sfu.ca/~keli/" target="_blank">Ke Li</a>,
        <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
        Advances in Neural Information Processing Systems (NeurIPS), 2024
        <p>
            Neural radiance fields (NeRFs) have gained popularity with multiple works showing promising results across various applications.
            However, to the best of our knowledge, existing works do not explicitly model the distribution of training camera poses, or consequently the triangulation quality,
            a key factor affecting reconstruction quality dating back to classical vision literature. 
            We close this gap with ProvNeRF, an approach that models the provenance for each point -- i.e., the locations where it is likely visible -- of NeRFs as a stochastic field. 
            We achieve this by extending implicit maximum likelihood estimation (IMLE) to functional space with an optimizable objective. 
            We show that modeling per-point provenance during the NeRF optimization enriches the model with information on triangulation leading to improvements in novel view synthesis and uncertainty estimation under the challenging sparse, 
            unconstrained view setting against competitive baselines.
        </p>
        <!-- <a href="bibtex/GeoConcept.txt" target="_blank">[BibTex]</a> -->
    </p></td>

<!--  Unified RoIAlign -->
<tr>
    <td width="30%" valign="top"><p><img src="papers/unifiedRoIAlign.png" width="300" alt="" style="border-style: none" align="top"></p></td>
    <td width="70%" valign="top"><p>
        <h3>
            Semantic-Aware Transformation-Invariant RoI Align
        </h3>
        <a href="https://scholar.google.com/citations?user=eiQ8AdkAAAAJ&hl=en" target="_blank">Guo-Ye Yang</a>,
        <strong>Kiyohiro Nakayama</strong>,
        Zi-Kai Xiao,
        Tai-Jiang Mu, 
        Sharon Xiaolei Huang,
        <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank">Shi-Min Hu</a><br>
        AAAI Conference on Artificial Intelligence, 2024
        <p>
            Great progress has been made in learning-based object detection methods in the last decade. Two-stage detectors often have higher detection accuracy than one-stage detectors,
            due to the use of region of interest (RoI) feature extractors
            which extract transformation-invariant RoI features for different RoI proposals, making refinement of bounding boxes
            and prediction of object categories more robust and accurate.
            However, previous RoI feature extractors can only extract invariant features under limited transformations. In this paper,
            we propose a novel RoI feature extractor, termed Semantic
            RoI Align (SRA), which is capable of extracting invariant RoI
            features under a variety of transformations for two-stage detectors. Specifically, we propose a semantic attention module
            to adaptively determine different sampling areas by leveraging the global and local semantic relationship within the RoI.
            We also propose a Dynamic Feature Sampler which dynamically samples features based on the RoI aspect ratio to enhance the efficiency of SRA, and a new position embedding,
            i.e., Area Embedding, to provide more accurate position information for SRA through an improved sampling area representation. Experiments show that our model significantly
            outperforms baseline models with slight computational overhead. In addition, it shows excellent generalization ability and
            can be used to improve performance with various state-ofthe-art backbones and detection methods.
        </p>
        <!-- <a href="bibtex/GeoConcept.txt" target="_blank">[BibTex]</a> -->
    </p></td>
</tr>
<!--  PL-NeRF -->
<tr>
<td width="30%" valign="top"><p><img src="papers/density.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        NeRF Revisited: Fixing Quadrature Instability in Volume Rendering
    </h3>
    <a href="https://mikacuy.github.io/" target="_blank">Mikaela Angelina Uy</a>,
    <strong>Kiyohiro Nakayama</strong>,
    <a href="https://www.guandaoyang.com/" target="_blank">Guangdao Yang</a>,
    Rahul Krishna, 
    <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a>,
    <a href="http://www.sfu.ca/~keli/" target="_blank">Ke Li</a><br>
    Advances in Neural Information Processing Systems (NeurIPS), 2023
    <p>
        Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under piecewise constant volume density. As a consequence, the rendered
        result is unstable w.r.t. the choice of samples along the ray, a phenomenon that
        we dub quadrature instability. We propose a mathematically principled solution
        by reformulating the sample-based rendering equation so that it corresponds to
        the exact integral under piecewise linear volume density. This simultaneously
        resolves multiple issues: conflicts between samples along different rays, imprecise
        hierarchical sampling, and non-differentiability of quantiles of ray termination
        distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric
        reconstruction, and stronger depth supervision. Our proposed formulation can be
        also be used as a drop-in replacement to the volume rendering equation for existing methods like NeRFs
    </p>
    <!-- <a href="bibtex/GeoConcept.txt" target="_blank">[BibTex]</a> -->
</p></td>
</tr>

<!--  DiffFacto -->
<tr>
    <td width="30%" valign="top"><p><img src="papers/difffacto.png" width="300" alt="" style="border-style: none" align="top"></p></td>
    <td width="70%" valign="top"><p>
        <h3>
            DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion
        </h3>
        <strong>Kiyohiro Nakayama</strong>,
        <a href="https://mikacuy.github.io/" target="_blank">Mikaela Angelina Uy</a>,
        <a href="https://cg.cs.tsinghua.edu.cn/people/~huangjh/" target="_blank">Jiahui Huang</a>,
        <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank">Shi-Min Hu</a>
        <a href="http://www.sfu.ca/~keli/" target="_blank">Ke Li</a>
        <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
        IEEE International Conference on Computer Vision (ICCV), 2023
        <p>
            We introduce DiffFacto, a novel probabilistic generative model that learns the distribution of shapes with part-level control. 
            We propose a factorization that models independent part style and part configuration distributions, 
            and present a novel cross diffusion network that enables us to generate coherent and plausible shapes under our proposed factorization. 
            Experiments show that our method is able to generate novel shapes with multiple axes of control. 
            It achieves state-of-the-art part-level generation quality and generates plausible and coherent shape, 
            while enabling various downstream editing applications such as shape interpolation, mixing and transformation editing.
        </p>
        <!-- <a href="bibtex/GeoConcept.txt" target="_blank">[BibTex]</a> -->
    </p></td>
    </tr>
<tr><td><br/></td></tr>
</table>
</ul>
</div>
<br>
<br>

<a name="talks"></a>
<h3>Invited Talks</h3>
<div class="section">
<ul>
    <li>[March 2023] DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion, at Stanford GCafe Seminar.</li>
</ul>
</div>
<br>

<a name="education"></a>
<h3>Education</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- Stanford -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/stanford.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Bachelor of Art in Mathematics, Stanford University</b><br/>2019.9 - 2024.6 (expected)<br/><br/>
        Academic Advisor: <a href="http://math.stanford.edu/~andras/" target='_blank'>Prof. Andras Vásy</a>
        Research Advisor: <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Prof. Leonidas J. Guibas</a>
        </p></td>
</tr>
<tr></tr>

<!-- Tsinghua -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/tsinghua.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Visiting Student, Graphics & Geometric Computing Group, Tsinghua University</b><br/>2022.2 - 2022.9<br/><br/>
        Advisor: <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank">Prof. Shi-Min Hu</a><br>
        </p></td>
</tr>
<tr></tr>


</table>
</ul>
</div>
<br>

<a name="experiences"></a>
<h3>Work Experiences</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- RIPS -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/ipam.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Undergraduate Researcher, Research in Industrial Projects for Students (RIPS), University of California, Los Angeles</b><br/>2022.6 - 2022.8<br/><br/>
        </p></td>
</tr>
<tr></tr>

</table>
</div>
</ul>
<br>


<a name="honor"></a>
<h3>Honors and Awards</h3>
<div class="section">
<ul>
    <li>[2017], Qualifier of USAMO</li>
</ul>
</div>
<br>


<hr/>
<div id="footer" style="font-size:10">George Nakayama <i>Last updated: Apr, 2023</i></div>
</body>
