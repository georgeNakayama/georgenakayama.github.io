<head>
    <title>Kiyohiro Nakayama</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="resources/main.css">
</head>

<body>

<table>
<tr>
<td><img src="images/avatar.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Kiyohiro (George) Nakayama 中山聖博</div>
<div>
Third Year Undergraduate at Stanford University<br/>
</div>
<div>
    <b>Email:</b> <tt>w4756677 [at] stanford [dot] com</tt><br>
</div>
<div>
<a href="resources/Resume.pdf" target='_blank'>[CV (April, 2023)]</a>&nbsp;
<a href="#project">[Projects]</a>&nbsp;
<a href="#education">[Education]</a>&nbsp;
<a href="#experiences">[Experiences]</a>&nbsp;
<br><br>
<!-- <a href="https://scholar.google.com/citations?user=pL7JsOsAAAAJ&hl=en" target='_blank'>[Google Scholar]</a>&nbsp; -->
<a href="https://github.com/georgeNakayama" target='_blank'>[GitHub]</a>
<a href="https://www.linkedin.com/in/kiyohiro-nakayama-473249123/" target="_blank">[LinkedIn]</a>
<!-- <a href="https://twitter.com/KaichunMo" target="_blank">[Twitter]</a> -->
</div>
</td>
</tr>
</table>
<br>

<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>"It isn't that we dream too wild a dream: <br>
        The trouble is we do not make it seem <br>
        Sufficiently unlikely; for the most <br>
        We can think up is a domestic ghost."</i></p>
        <p>--- Vladmir Nabokov <i>Pale Fire</i></p>
</div>

<h3>About</h3>
<div class="section">
<ul>
    <p>
        I am a third year undergraduate student at Stanford University majoring in mathematics.I am generally interested in computer vision, graphics and machine learning. My recent research focuses on the representation and generation of objects/scenes for 2D and 3D content creation. Specifically, I have worked on controllable shape generation and 2D image segmentation.
    </p>

    <p>
    I am fortunate to work in 
    <a href="https://geometry.stanford.edu/" target="_blank">The Stanford Geometric Computation Group</a> under 
    <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Prof. Leonadis J. Guibas</a>.

    In the lab I am mentored by 
    <a href="https://mikacuy.github.io/" target="_blank">Mikaela Angelina Uy</a>, whose help and support I cannot appreciate enough. 

    I also worked as a visiting student at 
<a href="https://www.tsinghua.edu.cn/en/" target='_blank'>Tsinghua University</a>
    under the supervision of 
    <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target='_blank'>Prof. Shi-Min Hu</a>.
    There, I am mentored by two wonderful mentors 
    <a href="https://cg.cs.tsinghua.edu.cn/people/~huangjh/" target="_blank">Jiahui Huang</a> and Guo-Wei Yang.
    </p>
</div>
<!-- <br>

<h3>News</h3>
<div class="section">
<ul>
<ul>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Apr, 2023] One paper accepted to ICML 2023.</li>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Mar, 2023] We will host <a href="https://ai3dcc.github.io/" target="_blank">The First Workshop on AI for 3D Content Creation (AI3DCC)</a> at ICCV 2023.</li>
    <li>[Feb, 2023] Call-for-Papers for CVPR 2023 workshops: 
            <a href="https://struco3d.github.io/cvpr2023" target="_blank">Structural and Compositional Learning on 3D Data (StruCo3D)</a>,
            <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">3D Vision and Robotics (3DVR)</a>, and 
            <a href="https://rhobin-challenge.github.io/" target="_blank">The Rhobin Challenge -- Reconstruction of human-object interaction</a>.
    </li>
    <li>[Feb, 2023] Papers accepted to CVPR 2023, ICLR 2023, TPAMI, and WACV 2023.</li>
    <li>[Jul, 2022] Two papers get accepted to ECCV 2022.</li>
    <li>[Jun, 2022] Successfully defended <a href="https://purl.stanford.edu/xn613mb1512" target="_blank">my Ph.D. thesis</a> (featured in <a href="https://www.rsipvision.com/ComputerVisionNews-2022June/22/" target="_blank">Computer Vision News magazine</a>).</li>
    <li>Our ECCV22 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/voli/" target="_blank">VOLI: Visual Object-oriented Learning meets Interaction: Discovery, Representations, and Applications</a>. Please consider submitting your works by July 5th, 2022. See you online in Oct, 2022.</li>
    <li>One paper gets accepted to CVPR 2022.</li>
    <li>[Jan, 2022] Three papers get accepted to ICLR 2022.</li>
    <li>[Sept, 2021] Two papers get accepted to CoRL 2021. My first ever papers in any Robotic Conference!</li>
    <li>[July, 2021] One paper gets accepted to ICCV 2021.</li>
    <li>[Apr, 2021] 
        Our ICCV2021 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/struco3d/" target="_blank">StruCo3D2021: Structural and Compositional Learning on 3D Data</a>. Please consider submitting your works by July 26, 2021. See you online on Oct 16th, 2021.</li>
    <li>[Sep, 2020] One paper gets accepted to NeurIPS 2020.</li>
    <li>[July, 2020] Two papers get accepted to ECCV 2020.</li>
    <li>[Feb, 2020] Two papers get accepted to CVPR 2020.</li>
    <li>[Dec, 2019] One paper gets accepted to ICLR 2020.</li>
    <li>[July, 2019] StructureNet gets accepted to Siggraph Asia 2019!</li>
    <li>[Feb, 2019] PartNet gets accepted to CVPR 2019!</li>
    <li>[February 2017] Our <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">PointNet</a> is accepted as an oral presentation in CVPR 2017, Honolulu, USA.</li>
</ul>
</div>
<br> -->




<a name="project"></a>
<h3>Research</h3>
<div class="mainsection">
<ul>

<!-- (<sup>*</sup>: indicates equal contribution.) -->

<h4 style="font-size: 20px;">2023</h4>
<table width="100%">

<!--  DiffFacto -->
<tr>
<td width="30%" valign="top"><p><img src="papers/difffacto.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion
    </h3>
    <strong>Kiyohiro Nakayama</strong>,
    <a href="https://mikacuy.github.io/" target="_blank">Mikaela Angelina Uy</a>,
    <a href="https://cg.cs.tsinghua.edu.cn/people/~huangjh/" target="_blank">Jiahui Huang</a>,
    <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank">Shi-Min Hu</a><br>
    <a href="http://www.sfu.ca/~keli/" target="_blank">Ke Li</a><br>
    <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonadis J. Guibas</a>
    In Submission
    <p>
        We introduce DiffFacto, a novel probabilistic generative model that learns the distribution of shapes with part-level control. 
        We propose a factorization that models independent part style and part configuration distributions, 
        and present a novel cross diffusion network that enables us to generate coherent and plausible shapes under our proposed factorization. 
        Experiments show that our method is able to generate novel shapes with multiple axes of control. 
        It achieves state-of-the-art part-level generation quality and generates plausible and coherent shape, 
        while enabling various downstream editing applications such as shape interpolation, mixing and transformation editing.
    </p>
    <!-- <a href="bibtex/GeoConcept.txt" target="_blank">[BibTex]</a> -->
</p></td>
</tr>

<tr><td><br/></td></tr>
</table>
</ul>
</div>
<br>
<br>

<a name="talks"></a>
<h3>Invited Talks</h3>
<div class="section">
<ul>
    <li>[March 2023] DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion, at Stanford GCafe Seminar.</li>
</ul>
</div>
<br>

<a name="education"></a>
<h3>Education</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- Stanford -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/stanford.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Bachelor of Art in Mathematics, Stanford University</b><br/>2019.9 - 2024.6 (expected)<br/><br/>
        Academic Advisor: <a href="http://math.stanford.edu/~andras/" target='_blank'>Prof. Andras Vásy</a>
        Research Advisor: <a href="https://geometry.stanford.edu/member/guibas/index.html" target="_blank">Prof. Leonadis J. Guibas</a>
        </p></td>
</tr>
<tr></tr>

<!-- Tsinghua -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/tsinghua.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Visiting Student, Graphics & Geometric Computing Group, Tsinghua University</b><br/>2022.2 - 2022.9<br/><br/>
        Advisor: <a href="https://cg.cs.tsinghua.edu.cn/shimin.htm" target="_blank">Prof. Shi-Min Hu</a><br>
        </p></td>
</tr>
<tr></tr>


</table>
</ul>
</div>
<br>

<a name="experiences"></a>
<h3>Work Experiences</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- RIPS -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/ipam.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Undergraduate Researcher, Research in Industrial Projects for Students (RIPS), University of California, Los Angeles</b><br/>2022.6 - 2022.8<br/><br/>
        </p></td>
</tr>
<tr></tr>

</table>
</div>
</ul>
<br>


<a name="honor"></a>
<h3>Honors and Awards</h3>
<div class="section">
<ul>
    <li>[2017], Qualifier of United States Mathematics Olympiad</li>
    <li>[2017], Perfect Score, American Mathematics Competition 12</li>
</ul>
</div>
<br>


<hr/>
<div id="footer" style="font-size:10">George Nakayama <i>Last updated: Apr, 2023</i></div>
</body>
